{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd8ec2e7-3588-476a-bf29-b1ac25e9b95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from config import *\n",
    "import os\n",
    "import json\n",
    "from scripts.run_active_learning import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9484750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "TOP5_SUMMARY = Top5_Similarity_Summary\n",
    "MIN_MAX_SUMMARY = EDX_min_max_summary\n",
    "MAPPED_CENTROIDS_JSON = MAPPED_CENTROIDS_JSON\n",
    "OUTPUT_DIR = DATA_CLEAN_InIT_CHOICES\n",
    "# Create output dir\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Load inputs\n",
    "top5_df = pd.read_csv(TOP5_SUMMARY)\n",
    "minmax_df = pd.read_csv(MIN_MAX_SUMMARY)\n",
    "with open(MAPPED_CENTROIDS_JSON, 'r') as f:\n",
    "    centroids_mapped = json.load(f)\n",
    "\n",
    "# Collect only short folders (e.g., 10268, 10269, etc.)\n",
    "short_folders = sorted(\n",
    "    set(top5_df['Folder'].astype(str)) |\n",
    "    set(minmax_df['Folder'].astype(str))\n",
    ")\n",
    "\n",
    "for folder in short_folders:\n",
    "    folder_indices = {}\n",
    "\n",
    "    # Top5 Similarity indices\n",
    "    top5_indices = top5_df[top5_df['Folder'].astype(str).str.startswith(folder)]['index'].tolist()\n",
    "    if top5_indices:\n",
    "        folder_indices['Top5Similarity'] = top5_indices\n",
    "\n",
    "    #  Max/Min Comp grouping\n",
    "    max_comp = []\n",
    "    min_comp = []\n",
    "\n",
    "    minmax_subset = minmax_df[minmax_df['Folder'].astype(str).str.startswith(folder)]\n",
    "    for _, row in minmax_subset.iterrows():\n",
    "        min_idx = row['MinIndex']\n",
    "        max_idx = row['MaxIndex']\n",
    "        min_comp.append(min_idx)\n",
    "        max_comp.append(max_idx)\n",
    "\n",
    "    if max_comp:\n",
    "        folder_indices['Max Comp'] = max_comp\n",
    "    if min_comp:\n",
    "        folder_indices['Min Comp'] = min_comp\n",
    "\n",
    "    # Centroids mapped\n",
    "    matching_wafer_id = f\"00{folder}\"\n",
    "    for wafer_key, centroids_dict in centroids_mapped.items():\n",
    "        if wafer_key.startswith(matching_wafer_id):\n",
    "            for sat_level, centroid_entries in centroids_dict.items():\n",
    "                nearest_indices = []\n",
    "                for entry in centroid_entries:\n",
    "                    if isinstance(entry, dict) and 'nearest_stage_index' in entry:\n",
    "                        nearest_indices.append(entry['nearest_stage_index'])\n",
    "\n",
    "                cleaned_key = f\"Centroids_{sat_level.replace(' ', '_')}\"\n",
    "                folder_indices[cleaned_key] = nearest_indices[:5]  # Limit to 5\n",
    "\n",
    "    #  Only save if we collected something\n",
    "    if folder_indices:\n",
    "        save_path = os.path.join(OUTPUT_DIR, f\"{folder}_indices.json\")\n",
    "        with open(save_path, 'w') as f:\n",
    "            json.dump(folder_indices, f, indent=4)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20b6ee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of datasets from config\n",
    "datasets = [\n",
    "    DATASET_10272_Ag_Au_Pd_RT,\n",
    "    DATASET_10275_Ag_Au_Pd_Pt_Rh_RT,\n",
    "    DATASET_10304_Au_Pd_Pt_Rh_RT,\n",
    "    DATASET_10311_Au_Pd_Pt_Rh_Ru_RT,\n",
    "    DATASET_10403_Ag_Au_Cu_Pd_Pt_RT,\n",
    "    DATASET_10402_Ag_Au_Pd_Pt_RT,\n",
    "    DATASET_10399_Au_Cu_Pd_Pt_RT,\n",
    "    DATASET_10374_Ir_Pd_Pt_Rh_Ru \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "143049f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing dataset: 10272\n",
      "Headers of DataFrame:\n",
      " ['ID' 'x' 'y' 'Ag' 'Au' 'Pd' 'Resistance']\n",
      "{'Top5Similarity': [323, 334, 310, 295, 278], 'Max Comp': [178, 23, 278], 'Min Comp': [114, 220, 94], 'Centroids_saturation_high': [130, 192, 252, 268, 103], 'Centroids_saturation_medium': [172, 233, 110, 83, 270], 'Centroids_saturation_low': [151, 231, 213, 268, 110], 'Random': [262, 149, 222, 30, 148], 'LHS': [82, 36, 127, 336, 44], 'K-Means': [5, 11, 0, 112, 105], 'Farthest': [32, 178, 278, 325, 339], 'ODAL': [10, 23, 38, 55, 74], 'K-Center': [180, 240, 5, 338, 38]}\n",
      "\n",
      " Processing dataset: 10275\n",
      "Headers of DataFrame:\n",
      " ['ID' 'x' 'y' 'Ag' 'Au' 'Pd' 'Pt' 'Rh' 'Resistance']\n",
      "{'Top5Similarity': [24, 11, 0, 12, 1], 'Max Comp': [220, 74, 295, 335, 0], 'Min Comp': [114, 220, 39, 74, 323], 'Centroids_saturation_high': [231, 124, 103, 145, 147], 'Centroids_saturation_medium': [125, 146, 83, 83, 231], 'Centroids_saturation_low': [104, 146, 83, 167, 231], 'Random': [262, 149, 222, 30, 148], 'LHS': [224, 128, 170, 143, 146], 'K-Means': [7, 56, 0, 171, 166], 'Farthest': [32, 260, 341, 177, 24], 'ODAL': [0, 38, 39, 55, 56], 'K-Center': [180, 114, 340, 5, 11]}\n",
      "\n",
      " Processing dataset: 10304\n",
      "Headers of DataFrame:\n",
      " ['ID' 'x' 'y' 'Au' 'Pd' 'Pt' 'Rh' 'Resistance']\n",
      "{'Top5Similarity': [94, 115, 199, 136, 178], 'Max Comp': [74, 278, 324, 24], 'Min Comp': [260, 24, 10, 278], 'Centroids_saturation_high': [125, 143, 252, 228, 140], 'Centroids_saturation_medium': [144, 103, 249, 167, 170], 'Centroids_saturation_low': [145, 185, 83, 268, 170], 'Random': [262, 149, 222, 30, 148], 'LHS': [129, 99, 140, 250, 270], 'K-Means': [22, 183, 0, 189, 4], 'Farthest': [32, 335, 240, 94, 241], 'ODAL': [0, 10, 11, 23, 24], 'K-Center': [180, 156, 338, 7, 187]}\n",
      "\n",
      " Processing dataset: 10311\n",
      "Headers of DataFrame:\n",
      " ['ID' 'x' 'y' 'Au' 'Pd' 'Pt' 'Rh' 'Ru' 'Resistance']\n",
      "{'Top5Similarity': [39, 24, 11, 0, 94], 'Max Comp': [135, 323, 324, 56, 4], 'Min Comp': [199, 11, 23, 295, 340], 'Centroids_saturation_high': [144, 168, 80, 290, 270], 'Centroids_saturation_medium': [144, 81, 103, 211, 168], 'Centroids_saturation_low': [188, 166, 63, 103, 273], 'Random': [262, 149, 222, 30, 148], 'LHS': [230, 121, 213, 187, 122], 'K-Means': [54, 184, 0, 209, 4], 'Farthest': [32, 336, 240, 199, 24], 'ODAL': [0, 1, 4, 10, 11], 'K-Center': [180, 177, 339, 8, 0]}\n",
      "\n",
      " Processing dataset: 10403\n",
      "Headers of DataFrame:\n",
      " ['ID' 'x' 'y' 'Ag' 'Au' 'Cu' 'Pd' 'Pt' 'Resistance']\n",
      "{'Top5Similarity': [324, 311, 312, 296, 297], 'Max Comp': [220, 0, 278, 55, 335], 'Min Comp': [198, 295, 56, 279, 10], 'Centroids_saturation_high': [146, 288, 103, 211, 251], 'Centroids_saturation_medium': [146, 251, 103, 190, 270], 'Centroids_saturation_low': [146, 190, 288, 289, 103], 'Random': [262, 149, 222, 30, 148], 'LHS': [234, 228, 64, 122, 146], 'K-Means': [7, 56, 0, 168, 166], 'Farthest': [32, 324, 295, 94, 74], 'ODAL': [0, 1, 8, 10, 11], 'K-Center': [180, 114, 340, 5, 167]}\n",
      "\n",
      " Processing dataset: 10402\n",
      "Headers of DataFrame:\n",
      " ['ID' 'x' 'y' 'Ag' 'Au' 'Pd' 'Pt' 'Resistance']\n",
      "{'Top5Similarity': [341, 334, 333, 340, 332], 'Max Comp': [220, 0, 114, 341], 'Min Comp': [135, 341, 260, 1], 'Centroids_saturation_high': [210, 232, 82, 126, 168], 'Centroids_saturation_medium': [251, 126, 83, 189, 269], 'Centroids_saturation_low': [168, 230, 288, 82, 147], 'Random': [262, 149, 222, 30, 148], 'LHS': [107, 170, 211, 168, 183], 'K-Means': [8, 127, 0, 56, 228], 'Farthest': [32, 335, 295, 157, 74], 'ODAL': [0, 1, 10, 56, 74], 'K-Center': [180, 114, 334, 6, 324]}\n",
      "\n",
      " Processing dataset: 10399\n",
      "Headers of DataFrame:\n",
      " ['ID' 'x' 'y' 'Au' 'Cu' 'Pd' 'Pt' 'Resistance']\n",
      "{'Top5Similarity': [324, 335, 311, 325, 312], 'Max Comp': [39, 278, 74, 324], 'Min Comp': [295, 56, 260, 10], 'Centroids_saturation_high': [106, 166, 265, 230, 172], 'Centroids_saturation_medium': [207, 105, 250, 149, 164], 'Centroids_saturation_low': [190, 105, 207, 248, 125], 'Random': [262, 149, 222, 30, 148], 'LHS': [140, 183, 33, 307, 188], 'K-Means': [6, 136, 0, 245, 125], 'Farthest': [32, 335, 157, 294, 135], 'ODAL': [0, 1, 11, 24, 39], 'K-Center': [180, 74, 323, 4, 324]}\n",
      "\n",
      " Processing dataset: 10374\n",
      "Headers of DataFrame:\n",
      " ['ID' 'x' 'y' 'Ru' 'Rh' 'Pd' 'Ir' 'Pt' 'Resistance']\n",
      "{'Top5Similarity': [23, 9, 10, 22, 38], 'Max Comp': [220, 0, 278, 55, 336], 'Min Comp': [156, 323, 0, 220, 10], 'Centroids_saturation_high': [148, 164, 143, 233, 208], 'Centroids_saturation_medium': [105, 168, 85, 123, 288], 'Centroids_saturation_low': [85, 187, 124, 148, 287], 'Random': [262, 149, 222, 30, 148], 'LHS': [32, 42, 78, 26, 117], 'K-Means': [7, 3, 0, 113, 101], 'Farthest': [32, 0, 115, 340, 220], 'ODAL': [0, 1, 4, 11, 12], 'K-Center': [180, 0, 156, 3, 94]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Helper to convert non-serializable types\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, np.bool_):\n",
    "        return bool(obj)\n",
    "    return obj\n",
    "\n",
    "for dataset_path in datasets:\n",
    "\n",
    "    dataset_name = os.path.basename(dataset_path).split(\"_\")[0]\n",
    "    print(f\"\\n Processing dataset: {dataset_name}\")\n",
    "    \n",
    "    data_exp = pd.read_csv(dataset_path)\n",
    "\n",
    "    if data_exp.empty:\n",
    "        print(f\" Warning: Dataset {dataset_name} is empty.\")\n",
    "        continue\n",
    "\n",
    "    json_path = os.path.join(DATA_CLEAN_InIT_CHOICES, f\"{dataset_name}_indices.json\")\n",
    "    #print(f\" Loading init strategies from: {json_path}\")\n",
    "\n",
    "    if not os.path.exists(json_path):\n",
    "        #print(f\" JSON file not found for {dataset_name}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with open(json_path, \"r\") as f:\n",
    "            init_choices = json.load(f)\n",
    "            #print(f\"Loaded init strategies: {init_choices}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\" JSON decode error in {json_path}: {e}\")\n",
    "        init_choices = {}\n",
    "\n",
    "\n",
    "    all_columns = data_exp.columns.tolist()\n",
    "    features = [col for col in all_columns if col not in [\"ID\", \"x\", \"y\", \"Resistance\"]]\n",
    "    target = [\"Resistance\"]\n",
    "\n",
    "    data_exp[target] = np.log(data_exp[target])\n",
    "\n",
    "    output_dir = os.path.join(UNCERTAINTY_PATH, dataset_name + \"_results\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    device = Resistance(data_exp, features=features, target=target)\n",
    "    X_all = device.get_features()\n",
    "    y_all = device.df[target[0]].values.reshape(-1, 1)\n",
    "    init_strategies = select_initial_indices(X_all, n_init=5)\n",
    "    init_choices.update(init_strategies)\n",
    "\n",
    "    renamed_init_choices = {}\n",
    "    for key, val in init_choices.items():\n",
    "        if \"Centroids_saturation_and_contrast_+++\" in key:\n",
    "            new_key = key.replace(\"Centroids_saturation_and_contrast_+++\", \"Centroids_saturation_high\")\n",
    "        elif \"Centroids_saturation_and_contrast_++\" in key:\n",
    "            new_key = key.replace(\"Centroids_saturation_and_contrast_++\", \"Centroids_saturation_medium\")\n",
    "        elif \"Centroids_saturation_and_contrast_+\" in key:\n",
    "            new_key = key.replace(\"Centroids_saturation_and_contrast_+\", \"Centroids_saturation_low\")\n",
    "        else:\n",
    "            new_key = key\n",
    "        renamed_init_choices[new_key] = val\n",
    "\n",
    "    init_choices = renamed_init_choices\n",
    "\n",
    "    print(init_choices)\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(init_choices, f, indent=4, default=convert_to_serializable)\n",
    "       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
